#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MPSä¼˜åŒ–ç‰ˆæ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰
ä¿®å¤PyTorch 2.6æƒé‡åŠ è½½é—®é¢˜
"""

import sys
import os
import argparse
from pathlib import Path
import torch

# å…¨å±€ä¿®å¤PyTorchæƒé‡åŠ è½½é—®é¢˜
def fix_pytorch_weights_loading():
    """å…¨å±€ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜"""
    import torch
    
    # ä¿å­˜åŸå§‹çš„torch.loadå‡½æ•°
    original_load = torch.load
    
    def patched_load(*args, **kwargs):
        """ä¿®å¤ç‰ˆçš„torch.loadå‡½æ•°"""
        # å¼ºåˆ¶è®¾ç½®weights_only=Falseä»¥é¿å…å®‰å…¨æ£€æŸ¥
        kwargs.setdefault('weights_only', False)
        return original_load(*args, **kwargs)
    
    # æ›¿æ¢torch.loadå‡½æ•°
    torch.load = patched_load
    print("âœ… PyTorchæƒé‡åŠ è½½ä¿®å¤å·²åº”ç”¨")

# åœ¨å¯¼å…¥ultralyticsä¹‹å‰å…ˆä¿®å¤
fix_pytorch_weights_loading()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.trainer import ModelTrainer
from src.utils.config_loader import config_loader


def setup_mps_optimization():
    """è®¾ç½®MPSä¼˜åŒ–ç¯å¢ƒ"""
    print("ğŸš€ è®¾ç½®Apple Silicon GPU (MPS) ä¼˜åŒ–...")
    
    # æ£€æŸ¥MPSå¯ç”¨æ€§
    if not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
        print("âŒ MPSè®¾å¤‡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯æ­è½½Apple SiliconèŠ¯ç‰‡çš„Mac")
        sys.exit(1)
    
    # MPSä¼˜åŒ–ç¯å¢ƒå˜é‡
    optimizations = {
        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',  # å†…å­˜ä¼˜åŒ–
        'PYTORCH_ENABLE_MPS_FALLBACK': '1',         # å¯ç”¨å›é€€æœºåˆ¶
        'MPS_CAPTURE_DEVICE_METRICS': '1',          # å¯ç”¨æ€§èƒ½ç›‘æ§
        'TORCH_WEIGHTS_ONLY': 'False',              # å…¨å±€ç¦ç”¨æƒé‡å®‰å…¨æ£€æŸ¥
    }
    
    for key, value in optimizations.items():
        os.environ[key] = value
        print(f"  âœ“ {key} = {value}")
    
    # æ£€æµ‹Apple Siliconç±»å‹
    try:
        import platform
        chip_info = platform.processor()
        print(f"  âœ“ æ£€æµ‹åˆ°èŠ¯ç‰‡: {chip_info}")
    except:
        pass
    
    print("âœ… MPSä¼˜åŒ–ç¯å¢ƒè®¾ç½®å®Œæˆ")


def get_optimal_batch_size(model_size='n'):
    """æ ¹æ®æ¨¡å‹å¤§å°æ¨èæœ€ä½³æ‰¹å¤„ç†å¤§å°"""
    batch_sizes = {
        'n': 32,   # YOLOv8n - è½»é‡çº§
        's': 24,   # YOLOv8s - å°å‹
        'm': 16,   # YOLOv8m - ä¸­å‹
        'l': 12,   # YOLOv8l - å¤§å‹
        'x': 8     # YOLOv8x - è¶…å¤§å‹
    }
    return batch_sizes.get(model_size, 16)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='MPSä¼˜åŒ–ç‰ˆYOLOv8åƒåœ¾åˆ†æ‹£æ¨¡å‹è®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰')
    parser.add_argument('--data', type=str, default='datasets/yolo_dataset/dataset.yaml', 
                       help='æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, default='yolov8n',
                       help='æ¨¡å‹åç§° (yolov8n, yolov8s, yolov8m, yolov8l, yolov8x)')
    parser.add_argument('--epochs', type=int, default=50,
                       help='è®­ç»ƒè½®æ•° (æ¨è: 50-100)')
    parser.add_argument('--batch', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å° (è‡ªåŠ¨ä¼˜åŒ–)')
    parser.add_argument('--auto-batch', action='store_true', default=True,
                       help='è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--lr', type=float, default=0.01,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=20,
                       help='æ—©åœè€å¿ƒå€¼')
    
    args = parser.parse_args()
    
    print("ğŸ" * 20)
    print("SmartBin åƒåœ¾åˆ†æ‹£ç³»ç»Ÿ - MPSåŠ é€Ÿè®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("ğŸ" * 20)
    
    # è®¾ç½®MPSä¼˜åŒ–
    setup_mps_optimization()
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    data_path = Path(args.data)
    if not data_path.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†: python scripts/prepare_data.py")
        sys.exit(1)
    
    # è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
    if args.auto_batch and args.batch is None:
        model_size = args.model[-1] if args.model.startswith('yolov8') else 'n'
        args.batch = get_optimal_batch_size(model_size)
        print(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°: {args.batch} (é’ˆå¯¹ {args.model})")
    
    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  ğŸ“ æ•°æ®é›†: {data_path}")
    print(f"  ğŸ¤– æ¨¡å‹: {args.model}")
    print(f"  ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch}")
    print(f"  ğŸ”„ è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  ğŸ“š å­¦ä¹ ç‡: {args.lr}")
    print(f"  â³ æ—©åœè€å¿ƒ: {args.patience}")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        print(f"\nğŸ—ï¸  åˆå§‹åŒ–è®­ç»ƒå™¨...")
        trainer = ModelTrainer()
        
        # éªŒè¯MPSè®¾å¤‡
        if trainer.device != 'mps':
            print(f"âš ï¸  è­¦å‘Š: é¢„æœŸä½¿ç”¨MPSä½†æ£€æµ‹åˆ°è®¾å¤‡: {trainer.device}")
        else:
            print(f"âœ… MPSè®¾å¤‡ç¡®è®¤: {trainer.device}")
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {args.model}")
        trainer.load_model(model_name=args.model, pretrained=True)
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸš€ å¼€å§‹MPSåŠ é€Ÿè®­ç»ƒ...")
        print(f"{'='*50}")
        
        # æ·»åŠ æ›´å¤šMPSä¼˜åŒ–å‚æ•°
        training_kwargs = {
            # MPSç‰¹å®šä¼˜åŒ–å‚æ•°
            'amp': True,        # è‡ªåŠ¨æ··åˆç²¾åº¦
            'cache': True,      # æ•°æ®ç¼“å­˜
            'workers': 0,       # MPSå•çº¿ç¨‹å·¥ä½œ
            'verbose': True,    # è¯¦ç»†è¾“å‡º
            'plots': True,      # ç”Ÿæˆè®­ç»ƒå›¾è¡¨
            'save': True,       # ä¿å­˜æ£€æŸ¥ç‚¹
            'save_period': min(args.epochs // 5, 10),  # åŠ¨æ€ä¿å­˜å‘¨æœŸ
            'cos_lr': True,     # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
            'close_mosaic': args.epochs - 10 if args.epochs > 10 else 0,  # å…³é—­é©¬èµ›å…‹å¢å¼º
        }
        
        results = trainer.train(
            data_config=str(data_path),
            epochs=args.epochs,
            batch_size=args.batch,
            learning_rate=args.lr,
            patience=args.patience,
            resume=False,
            **training_kwargs
        )
        
        print(f"{'='*50}")
        print(f"ğŸ‰ MPSåŠ é€Ÿè®­ç»ƒå®Œæˆ!")
        print(f"{'='*50}")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {results['best_model_path']}")
        print(f"ğŸ“Š æœ€æ–°æ¨¡å‹: {results['last_model_path']}")
        print(f"ğŸ“ ç»“æœç›®å½•: {results['results_dir']}")
        
        # æ˜¾ç¤ºè®­ç»ƒå›¾è¡¨ä½ç½®
        results_dir = Path(results['results_dir'])
        print(f"\nğŸ“ˆ è®­ç»ƒç»“æœæ–‡ä»¶:")
        
        result_files = [
            ('results.png', 'ğŸ“Š è®­ç»ƒæ›²çº¿å›¾'),
            ('confusion_matrix.png', 'ğŸ”¢ æ··æ·†çŸ©é˜µ'),
            ('labels.jpg', 'ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ'),
            ('train_batch0.jpg', 'ğŸ–¼ï¸ è®­ç»ƒæ ·æœ¬'),
            ('results.csv', 'ğŸ“‹ è¯¦ç»†æ•°æ®')
        ]
        
        for filename, description in result_files:
            filepath = results_dir / filename
            if filepath.exists():
                print(f"  {description}: {filepath}")
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶
        best_model = Path(results['best_model_path'])
        last_model = Path(results['last_model_path'])
        
        if best_model.exists() and last_model.exists():
            print(f"\nâœ… æ¨¡å‹æ–‡ä»¶éªŒè¯:")
            print(f"  ğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model.stat().st_size / 1024 / 1024:.1f}MB")
            print(f"  ğŸ”„ æœ€æ–°æ¨¡å‹: {last_model.stat().st_size / 1024 / 1024:.1f}MB")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print(f"  â€¢ ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†æµ‹è¯•")
        print(f"  â€¢ éƒ¨ç½²åˆ°Webç•Œé¢æˆ–åµŒå…¥å¼è®¾å¤‡")
        print(f"  â€¢ å¯¼å‡ºä¸ºONNXæˆ–TensorRTæ ¼å¼")
        print(f"  â€¢ åœ¨çœŸå®åœºæ™¯ä¸­éªŒè¯æ€§èƒ½")
        
        print(f"\nğŸ Apple Silicon GPUåŠ é€Ÿè®­ç»ƒæˆåŠŸå®Œæˆ!")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 