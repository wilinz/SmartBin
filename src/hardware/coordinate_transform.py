#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åæ ‡è½¬æ¢æ¨¡å—
åŸºäº uarm_demo/uarm_demo.py ä¸­çš„ TransForm ç±»å®ç°
ç”¨äºå°†å›¾åƒåæ ‡è½¬æ¢ä¸ºæœºæ¢°è‡‚åæ ‡
"""

import numpy as np
import cv2
from typing import Tuple, Optional, List


class CoordinateTransform:
    """
    åæ ‡è½¬æ¢ç±»
    
    ä½¿ç”¨å•åº”æ€§çŸ©é˜µï¼ˆHomography Matrixï¼‰è¿›è¡Œå›¾åƒåæ ‡åˆ°æœºæ¢°è‡‚åæ ‡çš„è½¬æ¢
    """
    
    def __init__(self, camera_coordinates=None, robot_coordinates=None):
        """
        åˆå§‹åŒ–åæ ‡è½¬æ¢å™¨
        
        Args:
            camera_coordinates: å›¾åƒå››ä¸ªè§’ç‚¹çš„åæ ‡ (å·¦ä¸Š, å³ä¸Š, å³ä¸‹, å·¦ä¸‹)
            robot_coordinates: æœºæ¢°è‡‚å¯¹åº”å››ä¸ªç‚¹çš„åæ ‡ (å·¦ä¸Š, å³ä¸Š, å³ä¸‹, å·¦ä¸‹)
        """
        # é»˜è®¤ä½¿ç”¨ uarm_demo.py ä¸­çš„åæ ‡ç‚¹
        self.camera_points = np.array([
            [0, 0],     # å·¦ä¸Š
            [640, 0],   # å³ä¸Š
            [640, 480], # å³ä¸‹
            [0, 480]    # å·¦ä¸‹
        ], dtype=np.float32) if camera_coordinates is None else np.array(camera_coordinates, dtype=np.float32)
        
        self.robot_points = np.array([
            [91.3, -99.5],   # å·¦ä¸Š
            [88.4, 35.5],    # å³ä¸Š
            [205.7, 40.9],   # å³ä¸‹
            [211.5, -120.2]  # å·¦ä¸‹
        ], dtype=np.float32) if robot_coordinates is None else np.array(robot_coordinates, dtype=np.float32)
        
        # è®¡ç®—å•åº”æ€§çŸ©é˜µ
        self.H, _ = cv2.findHomography(self.camera_points, self.robot_points)
        print(f"âœ… å•åº”æ€§çŸ©é˜µè®¡ç®—å®Œæˆ:\n{self.H}")
        
        # è®¡ç®—å›¾åƒä¸­å¿ƒåœ¨æœºæ¢°è‡‚åæ ‡ç³»ä¸­çš„ä½ç½®
        self.center_point = self.convert_coordinate(320, 240)
        print(f"âœ… å›¾åƒä¸­å¿ƒåœ¨æœºæ¢°è‡‚åæ ‡ç³»ä¸­çš„ä½ç½®: {self.center_point}")
    
    def convert_coordinate(self, x: float, y: float) -> Tuple[float, float]:
        """
        å°†å›¾åƒåæ ‡è½¬æ¢ä¸ºæœºæ¢°è‡‚åæ ‡
        
        Args:
            x: å›¾åƒxåæ ‡
            y: å›¾åƒyåæ ‡
            
        Returns:
            tuple: (æœºæ¢°è‡‚xåæ ‡, æœºæ¢°è‡‚yåæ ‡)
        """
        # ä½¿ç”¨å•åº”æ€§çŸ©é˜µè½¬æ¢åæ ‡
        point = np.array([[x, y]], dtype=np.float32)
        transformed = cv2.perspectiveTransform(point.reshape(1, -1, 2), self.H)
        
        # è¿”å›è½¬æ¢åçš„åæ ‡
        return float(transformed[0][0][0]), float(transformed[0][0][1])
    
    def get_coordinate(self, img, x: float, y: float) -> Tuple[float, float]:
        """
        ä»å›¾åƒå’Œå›¾åƒä¸­çš„ç‚¹è·å–æœºæ¢°è‡‚åæ ‡
        
        Args:
            img: å›¾åƒï¼ˆç”¨äºè·å–å°ºå¯¸ä¿¡æ¯ï¼Œä½†è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨åæ ‡ç‚¹ï¼‰
            x: å›¾åƒxåæ ‡
            y: å›¾åƒyåæ ‡
            
        Returns:
            tuple: (æœºæ¢°è‡‚xåæ ‡, æœºæ¢°è‡‚yåæ ‡)
        """
        return self.convert_coordinate(x, y)
    
    def convert_multiple_points(self, points: List[Tuple[float, float]]) -> List[Tuple[float, float]]:
        """
        æ‰¹é‡è½¬æ¢å¤šä¸ªåæ ‡ç‚¹
        
        Args:
            points: å›¾åƒåæ ‡ç‚¹åˆ—è¡¨ [(x1, y1), (x2, y2), ...]
            
        Returns:
            list: æœºæ¢°è‡‚åæ ‡ç‚¹åˆ—è¡¨ [(x1, y1), (x2, y2), ...]
        """
        if not points:
            return []
        
        # å°†ç‚¹è½¬æ¢ä¸ºnumpyæ•°ç»„
        points_array = np.array(points, dtype=np.float32).reshape(-1, 1, 2)
        
        # æ‰¹é‡è½¬æ¢
        transformed = cv2.perspectiveTransform(points_array, self.H)
        
        # è½¬æ¢å›åˆ—è¡¨æ ¼å¼
        result = []
        for point in transformed:
            result.append((float(point[0][0]), float(point[0][1])))
        
        return result
    
    def update_calibration(self, camera_points: List[Tuple[float, float]], 
                          robot_points: List[Tuple[float, float]]) -> bool:
        """
        æ›´æ–°æ ‡å®šå‚æ•°
        
        Args:
            camera_points: æ–°çš„å›¾åƒåæ ‡ç‚¹
            robot_points: æ–°çš„æœºæ¢°è‡‚åæ ‡ç‚¹
            
        Returns:
            bool: æ›´æ–°æˆåŠŸè¿”å›True
        """
        try:
            self.camera_points = np.array(camera_points, dtype=np.float32)
            self.robot_points = np.array(robot_points, dtype=np.float32)
            
            # é‡æ–°è®¡ç®—å•åº”æ€§çŸ©é˜µ
            self.H, _ = cv2.findHomography(self.camera_points, self.robot_points)
            
            # é‡æ–°è®¡ç®—å›¾åƒä¸­å¿ƒä½ç½®
            self.center_point = self.convert_coordinate(320, 240)
            
            print(f"âœ… æ ‡å®šå‚æ•°æ›´æ–°å®Œæˆ")
            print(f"âœ… æ–°çš„å•åº”æ€§çŸ©é˜µ:\n{self.H}")
            print(f"âœ… æ–°çš„å›¾åƒä¸­å¿ƒä½ç½®: {self.center_point}")
            
            return True
        except Exception as e:
            print(f"âŒ æ ‡å®šå‚æ•°æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def get_transform_matrix(self) -> np.ndarray:
        """
        è·å–å•åº”æ€§çŸ©é˜µ
        
        Returns:
            np.ndarray: å•åº”æ€§çŸ©é˜µ
        """
        return self.H.copy()
    
    def get_calibration_points(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        è·å–æ ‡å®šç‚¹
        
        Returns:
            tuple: (å›¾åƒåæ ‡ç‚¹, æœºæ¢°è‡‚åæ ‡ç‚¹)
        """
        return self.camera_points.copy(), self.robot_points.copy()
    
    def validate_transform(self) -> bool:
        """
        éªŒè¯åæ ‡è½¬æ¢çš„æœ‰æ•ˆæ€§
        
        Returns:
            bool: è½¬æ¢æœ‰æ•ˆè¿”å›True
        """
        try:
            # æµ‹è¯•è½¬æ¢å‡ ä¸ªå…³é”®ç‚¹
            test_points = [
                (0, 0),       # å·¦ä¸Šè§’
                (640, 0),     # å³ä¸Šè§’
                (640, 480),   # å³ä¸‹è§’
                (0, 480),     # å·¦ä¸‹è§’
                (320, 240)    # ä¸­å¿ƒç‚¹
            ]
            
            transformed = self.convert_multiple_points(test_points)
            
            print("ğŸ” åæ ‡è½¬æ¢éªŒè¯:")
            for i, (original, transformed_point) in enumerate(zip(test_points, transformed)):
                print(f"  å›¾åƒåæ ‡ {original} -> æœºæ¢°è‡‚åæ ‡ {transformed_point}")
            
            return True
        except Exception as e:
            print(f"âŒ åæ ‡è½¬æ¢éªŒè¯å¤±è´¥: {e}")
            return False
    
    def is_point_in_workspace(self, robot_x: float, robot_y: float, 
                             workspace_bounds: Optional[dict] = None) -> bool:
        """
        æ£€æŸ¥æœºæ¢°è‡‚åæ ‡æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…
        
        Args:
            robot_x: æœºæ¢°è‡‚xåæ ‡
            robot_y: æœºæ¢°è‡‚yåæ ‡
            workspace_bounds: å·¥ä½œç©ºé—´è¾¹ç•Œ {'x_min': ?, 'x_max': ?, 'y_min': ?, 'y_max': ?}
            
        Returns:
            bool: åœ¨å·¥ä½œç©ºé—´å†…è¿”å›True
        """
        if workspace_bounds is None:
            # é»˜è®¤å·¥ä½œç©ºé—´è¾¹ç•Œï¼ˆåŸºäºuArmçš„å·¥ä½œèŒƒå›´ï¼‰
            workspace_bounds = {
                'x_min': 0,
                'x_max': 300,
                'y_min': -150,
                'y_max': 150
            }
        
        return (workspace_bounds['x_min'] <= robot_x <= workspace_bounds['x_max'] and
                workspace_bounds['y_min'] <= robot_y <= workspace_bounds['y_max'])
    
    def get_safe_coordinate(self, x: float, y: float) -> Optional[Tuple[float, float]]:
        """
        è·å–å®‰å…¨çš„æœºæ¢°è‡‚åæ ‡ï¼ˆç¡®ä¿åœ¨å·¥ä½œç©ºé—´å†…ï¼‰
        
        Args:
            x: å›¾åƒxåæ ‡
            y: å›¾åƒyåæ ‡
            
        Returns:
            tuple: å®‰å…¨çš„æœºæ¢°è‡‚åæ ‡ï¼Œå¦‚æœè¶…å‡ºå·¥ä½œç©ºé—´åˆ™è¿”å›None
        """
        robot_x, robot_y = self.convert_coordinate(x, y)
        
        if self.is_point_in_workspace(robot_x, robot_y):
            return robot_x, robot_y
        else:
            print(f"âš ï¸ åæ ‡ ({robot_x}, {robot_y}) è¶…å‡ºå·¥ä½œç©ºé—´")
            return None


def test_coordinate_transform():
    """æµ‹è¯•åæ ‡è½¬æ¢åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åæ ‡è½¬æ¢åŠŸèƒ½")
    print("=" * 30)
    
    # åˆ›å»ºåæ ‡è½¬æ¢å™¨
    transformer = CoordinateTransform()
    
    # éªŒè¯è½¬æ¢
    transformer.validate_transform()
    
    # æµ‹è¯•å•ç‚¹è½¬æ¢
    print("\nğŸ“ æµ‹è¯•å•ç‚¹è½¬æ¢:")
    test_points = [
        (100, 100),
        (200, 200),
        (300, 300),
        (400, 400)
    ]
    
    for x, y in test_points:
        robot_x, robot_y = transformer.convert_coordinate(x, y)
        safe_coord = transformer.get_safe_coordinate(x, y)
        print(f"  å›¾åƒåæ ‡ ({x}, {y}) -> æœºæ¢°è‡‚åæ ‡ ({robot_x:.2f}, {robot_y:.2f})")
        if safe_coord:
            print(f"    âœ… åœ¨å·¥ä½œç©ºé—´å†…")
        else:
            print(f"    âŒ è¶…å‡ºå·¥ä½œç©ºé—´")
    
    # æµ‹è¯•æ‰¹é‡è½¬æ¢
    print("\nğŸ“ æµ‹è¯•æ‰¹é‡è½¬æ¢:")
    batch_points = [(50, 50), (150, 150), (250, 250)]
    transformed_batch = transformer.convert_multiple_points(batch_points)
    
    for original, transformed in zip(batch_points, transformed_batch):
        print(f"  {original} -> ({transformed[0]:.2f}, {transformed[1]:.2f})")
    
    print("\nâœ… åæ ‡è½¬æ¢æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_coordinate_transform() 